{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from src.dataset.eeg_dataset import EEGDataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from src.utils import Utils\n",
    "from src.analysis.interpretability import compute_gradcam_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ebe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [128, 2 * 128, 5 * 128, 10 * 128, 30 * 128, 60*128]\n",
    "step_sizes = [128, 2 * 128, 5 * 128, 10 * 128, 30 * 128, 60*128]\n",
    "\n",
    "window_size = window_sizes[3]\n",
    "step_size = step_sizes[3]\n",
    "preprocessing = True\n",
    "feature_selection = False\n",
    "depth_of_anesthesia = True\n",
    "for_majority = int(window_size / 2)\n",
    "strategy = \"rawEEG\"  \n",
    "sampling_rate = 128\n",
    "training_ids=[1, 2, 3, 4, 6, 7, 12,]\n",
    "random_seed=42\n",
    "\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_hdf(data_path / f\"training_data_{strategy}_{window_size}_{step_size}.h5\", key='eeg_window')\n",
    "test_data = pd.read_hdf(data_path / f\"test_data_{strategy}_{window_size}_{step_size}.h5\", key='eeg_window')\n",
    "validation_data = pd.read_hdf(data_path / f\"validation_data_{strategy}_{window_size}_{step_size}.h5\", key='eeg_window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils(\n",
    "    for_majority=for_majority,\n",
    "    window_size=window_size,\n",
    "    step_size=step_size,\n",
    "    random_seed=random_seed,\n",
    "    preprocessing=preprocessing,\n",
    "    sampling_rate=sampling_rate,\n",
    "    results_validation_csv_path=base_path\n",
    "    / \"doA_classification\"\n",
    "    / \"ml_models\"\n",
    "    / \"validation_results_df.csv\",\n",
    "    results_test_csv_path=base_path\n",
    "    / \"doA_classification\"\n",
    "    / \"ml_models\"\n",
    "    / \"test_results_df.csv\",\n",
    "    model_dir=base_path / \"doA_classification\" / \"ml_models\",\n",
    ")\n",
    "\n",
    "exclude_columns = [\"Start\", \"End\", \"sleep\"]\n",
    "labels = [\"sleep\"]\n",
    "if depth_of_anesthesia:\n",
    "    exclude_columns.extend([\"cr\", \"sspl\", \"burst_suppression\"])\n",
    "    labels_to_process = [\"sleep\", \"cr\", \"sspl\", \"burst_suppression\"]\n",
    "    labels.extend([\"cr\", \"sspl\", \"burst_suppression\"])\n",
    "else:\n",
    "    labels_to_process = [\"sleep\"]\n",
    "\n",
    "# Define features (excluding the necessary columns)\n",
    "features = training_data.drop(columns=exclude_columns, errors=\"ignore\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac83edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store preprocessed data\n",
    "preprocessed_data_dict = {}\n",
    "\n",
    "\n",
    "for label in labels_to_process:\n",
    "    print(f\"Processing {label}...\")\n",
    "    # Preprocess data\n",
    "    (\n",
    "        X,\n",
    "        y,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        train_loader_nn,\n",
    "        val_loader_nn,\n",
    "        test_loader_nn,\n",
    "        input_size,\n",
    "    ) = utils.preprocess_data(\n",
    "        X=np.vstack(training_data['eeg_window'].values),\n",
    "        y=training_data[label],\n",
    "        X_val=np.vstack(validation_data['eeg_window'].values),\n",
    "        y_val=validation_data[label],\n",
    "        X_test=np.vstack(test_data['eeg_window'].values),\n",
    "        y_test=test_data[label],\n",
    "        batch_size=16,\n",
    "        device=\"mps\",\n",
    "        strategy=strategy,\n",
    "        classification_type=label,\n",
    "        scaling_nn=False,\n",
    "        imbalanced=True,\n",
    "    )\n",
    "\n",
    "    preprocessed_data_dict[label] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"train_loader_nn\": train_loader_nn,\n",
    "        \"val_loader_nn\": val_loader_nn,\n",
    "        \"test_loader_nn\": test_loader_nn,\n",
    "        \"input_size\": input_size,\n",
    "    }\n",
    "\n",
    "print(\"Processing completed for all labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.UCR import UCRResNet\n",
    "\n",
    "task=\"burst_suppression\"\n",
    "model_name=\"UCRResNet\"\n",
    "model = UCRResNet(\n",
    "        input_shape=1,\n",
    "        n_feature_maps=64,\n",
    "        nb_classes=1\n",
    "    )\n",
    "# Assuming model, visu_x, and other variables are already defined\n",
    "model_filename = ( base_path / \"doA_classification\" / \"ml_models\" /\n",
    "    f\"rawEEG_{task}_ws{window_size}_ss{step_size}_\"\n",
    "    f\"majority{for_majority}_type{model_name}_\"\n",
    "    f\"preproc{preprocessing}_randomseed{random_seed}_all\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(780,859):\n",
    "    visu_x = preprocessed_data_dict[\"sleep\"][\"X_test\"][idx]\n",
    "    input_tensor = torch.Tensor(visu_x).unsqueeze(0)\n",
    "    if input_tensor.dim() == 2:\n",
    "        input_tensor = input_tensor.unsqueeze(1)\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{model_filename}.pt\", map_location='mps')\n",
    "    )\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    output = model(input_tensor)\n",
    "    if output.item() > 0.9:\n",
    "        print(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window 10\n",
    "#idx = 227, 229, 232, 234-> burst\n",
    "idx=827\n",
    "\n",
    "print(preprocessed_data_dict[\"sleep\"][\"y_test\"][idx])\n",
    "print(preprocessed_data_dict[\"sspl\"][\"y_test\"][idx])\n",
    "print(preprocessed_data_dict[\"cr\"][\"y_test\"][idx])\n",
    "print(preprocessed_data_dict[\"burst_suppression\"][\"y_test\"][idx])\n",
    "visu_x = preprocessed_data_dict[\"sleep\"][\"X_test\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visu_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eaad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_ws=64\n",
    "stft_hop=16\n",
    "window = torch.hann_window(stft_ws)\n",
    "stft_result = torch.stft(\n",
    "    torch.tensor(torch.tensor(visu_x)),\n",
    "    n_fft=stft_ws,\n",
    "    hop_length=stft_hop,\n",
    "    win_length=stft_ws,\n",
    "    return_complex=True,\n",
    "    window=window,\n",
    ")\n",
    "#stft_result = torch.view_as_real(stft_result)\n",
    "stft_result = 20 * torch.log10(stft_result + 1e-6)\n",
    "print(stft_result.shape)\n",
    "min_freq = 0.5\n",
    "max_freq = 47\n",
    "freqs = np.fft.rfftfreq(stft_ws, d=1 / 128)\n",
    "print(freqs)\n",
    "print(freqs.shape)\n",
    "cropped_stft = stft_result[\n",
    "    np.argmax(freqs >= min_freq) : np.argmax(freqs > max_freq)\n",
    "]\n",
    "\n",
    "print(stft_result.shape)\n",
    "print(cropped_stft.shape)\n",
    "cropped_stft = torch.view_as_real(cropped_stft)\n",
    "#print(cropped_stft[:, :, 0].shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the original image and the Grad-CAM heatmap\n",
    "plt.figure()  # Adjusted figsize for vertical layout\n",
    "plt.subplot(2, 1, 1)  # Changed to 2 rows, 1 column, 1st subplot\n",
    "plt.imshow(cropped_stft[:, :, 0], cmap='viridis')\n",
    "plt.title('Original Image')\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)  # Changed to 2 rows, 1 column, 2nd subplot\n",
    "plt.imshow(cropped_stft[:, :, 1], cmap='viridis')\n",
    "plt.title('Grad-CAM Heatmap')\n",
    "#plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, BinaryClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import Normalize\n",
    "from src.models.UCR import UCRResNet\n",
    "\n",
    "task=\"burst_suppression\"\n",
    "\n",
    "model_name=\"UCRResNet\"\n",
    "model = UCRResNet(\n",
    "        input_shape=1,\n",
    "        n_feature_maps=64,\n",
    "        nb_classes=1\n",
    "    )\n",
    "# Assuming model, visu_x, and other variables are already defined\n",
    "model_filename = ( base_path / \"doA_classification\" / \"ml_models\" /\n",
    "    f\"rawEEG_{task}_ws{window_size}_ss{step_size}_\"\n",
    "    f\"majority{for_majority}_type{model_name}_\"\n",
    "    f\"preproc{preprocessing}_randomseed{random_seed}_all\"\n",
    ")\n",
    "\n",
    "input_tensor = torch.Tensor(visu_x).unsqueeze(0)\n",
    "print(input_tensor.shape)\n",
    "if input_tensor.dim() == 2:\n",
    "    input_tensor = input_tensor.unsqueeze(1)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{model_filename}.pt\", map_location='mps')\n",
    ")\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "\n",
    "\n",
    "input_tensor.requires_grad_(True)\n",
    "output = model(input_tensor)\n",
    "print(\"Model output:\", output)\n",
    "output.backward()\n",
    "print(\"Input tensor gradients:\", input_tensor.grad)\n",
    "\n",
    "\n",
    "print(input_tensor.shape)\n",
    "input_tensor.requires_grad_(True)\n",
    "\n",
    "target_layer = model.conv9  # or any deeper layer\n",
    "input_tensor.requires_grad_(True)\n",
    "input_tensor = input_tensor.to('cpu')  # ensure same device\n",
    "grayscale_cam = compute_gradcam_1d(model, input_tensor, target_layer, target_class=0)\n",
    "\n",
    "\n",
    "print(grayscale_cam)\n",
    "\n",
    "grayscale_cam_1d = grayscale_cam.flatten()\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Create a set of line segments so that we can color them individually\n",
    "points = np.array([np.arange(len(visu_x)), visu_x]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "# Create a LineCollection from the segments\n",
    "lc = LineCollection(segments, cmap='viridis', norm=Normalize(vmin=grayscale_cam_1d.min(), vmax=grayscale_cam_1d.max()))\n",
    "lc.set_array(grayscale_cam_1d)\n",
    "lc.set_linewidth(2)\n",
    "line = ax.add_collection(lc)\n",
    "\n",
    "ax.set_xlim(0, len(visu_x))\n",
    "ax.set_ylim(visu_x.min(), visu_x.max())\n",
    "ax.set_title('EEG with GradCAM Overlay')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Amplitude')\n",
    "\n",
    "# Add a colorbar for the line plot\n",
    "cbar = fig.colorbar(line, ax=ax, label='Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72959c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
